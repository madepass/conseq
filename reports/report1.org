#+title: Report 1
#+PROPERTY: header-args:jupyter-python :session Report1
#+PROPERTY: header-args:jupyter-python+ :async yes
#+PROPERTY: header-args:jupyter-python+ :exports results
#+OPTIONS: author:nil

* Research question and hypotheses
* Determining Gain (g) and Difficulty (d)
Gain and difficulty are two of the most important parameters in the task. They determine the magnitude (and sign) of consequence as well as the magnitude of the difference between stimuli presented in a given trial. These parameters ultimately determine the optimal strategy.

The selection of g and d should allow us to determine whether participants' decisions are primarily determined by 1) value comparison or by 2) consequence. Moreover, in the case of value comparison, these parameters should enable us to distinguish classical Q-learning from Q-foraging.
*** Original values
The previous g was 0.3 for Horizon 1. The previous d values were: [0.01, 0.05, 0.1, 0.15, 0.2].

These values of g and d meant that Small-Big was always the optimal action sequence/policy in Horizon 1.

*** Updated values
More values of g, along with appropriate values of d, are required in order to determine how humans decide in the consequential task. I propose four values of g:

#+NAME: Table 1
#+ATTR_ODT: :rel-width 50
|-----------+-------+------+---------------------|
| Condition | G     |    g | \pi*                  |
|-----------+-------+------+---------------------|
| A         | G < 0 | -0.3 | Big-big             |
| B         | G = 0 |    0 | Big-big             |
| C         | G = c |  0.1 | Small-big ~ Big-big |
| D         | G > c |  0.3 | Small-big           |

I propose the following d values: [0.05, 0.2, 0.35]

#+begin_src  jupyter-python
import random
import numpy as np
import pandas as pd

def generate_stimuli(state, m, d, g):
    if state == 0:
        stimuli = [m - d / 2, m + d / 2]
    elif state == 1:
        stimuli = [m - g - d / 2, m - g + d / 2]
    else:  # state == 2
        stimuli = [m + g - d / 2, m + g + d / 2]
    random.shuffle(stimuli)  # randomize left or right position on screen
    return stimuli


def generate_cumulative_rewards(gs, difficulty):
    data = []
    for g in gs:
        d_data = []
        for d in difficulty:
            stimuli_trial_1 = generate_stimuli(0, m, d, g)
            stimuli_trial_2_bb = generate_stimuli(1, m, d, g)
            stimuli_trial_2_sb = generate_stimuli(2, m, d, g)
            small_big_cum_reward = min(stimuli_trial_1) + max(stimuli_trial_2_sb)
            big_big_cum_reward = max(stimuli_trial_1) + max(stimuli_trial_2_bb)
            cum_rewards = (round(small_big_cum_reward, 2), round(big_big_cum_reward, 2))
            d_data.append(cum_rewards)
        data.append(d_data)
    df = pd.DataFrame(
        data, columns=list(map(str, difficulty)), index=list(map(str, gs))
    )
    print("(cum reward small-big, cum reward big-big)")
    print(df, "\n")
    difference_df = df.applymap(lambda tup: tup[0] - tup[1])
    print("(cum reward small-big) - (sum reward big-big)")
    print(difference_df)


def min_and_max_stimuli(ms, difficulty, g):
    max_d = max(difficulty)
    min_stim = min(ms) - max(gs) - max_d / 2
    max_stim = max(ms) + max(gs) + max_d / 2
    print(f"({min_stim}, {max_stim})")


def get_ms(g, difficulty, n):
    ms = np.linspace(abs(g) + max(difficulty) / 2, 1 - abs(g) - max(difficulty) / 2, 5)
    return ms
#+end_src

#+RESULTS:


#+begin_src jupyter-python
m = 0.5 # m not relevant for calculating cumulative reward
gs = [-0.3, 0, 0.1, 0.3]
difficulty = [0.05, 0.2, 0.35]

print("g: ", gs)
print("difficulty: ", difficulty)
print("\n")
generate_cumulative_rewards(gs, difficulty)
#+end_src

#+RESULTS:
#+begin_example
g:  [-0.3, 0, 0.1, 0.3]
difficulty:  [0.05, 0.2, 0.35]


(cum reward small-big, cum reward big-big)
             0.05         0.2         0.35
-0.3  (0.7, 1.35)  (0.7, 1.5)  (0.7, 1.65)
0     (1.0, 1.05)  (1.0, 1.2)  (1.0, 1.35)
0.1   (1.1, 0.95)  (1.1, 1.1)  (1.1, 1.25)
0.3   (1.3, 0.75)  (1.3, 0.9)  (1.3, 1.05)

(cum reward small-big) - (sum reward big-big)
      0.05  0.2  0.35
-0.3 -0.65 -0.8 -0.95
0    -0.05 -0.2 -0.35
0.1   0.15  0.0 -0.15
0.3   0.55  0.4  0.25
#+end_example

The second "table" above shows shows the difference in cumulative reward, for all g and d, between the Small-Big and Big-Big action sequences. These values of g and d yield the optimal strategies outlined in Table 1. For these proposed values, the optimal policy is Big-Big for g=-0.3 and g=0. For g=0.1, Small-Big and Big-big yield identical cumulative reward.  The optimal strategy, in this case, is Small-Big when d is large and Big-Big when d is small. Small-Big and Big-Big yield identical cumulative reward when d is 0.2 (the intermediate value). Finally, the optimal strategy is Small-Big when g=0.3.
* Differences between Horizon 1 v1 & Horizon 1 v2
Table 2 shows most of the important differences between version 1 and version 2 of the task.
#+NAME: Table 2
#+ATTR_ODT: :rel-width 50
| attribute         | v1                           | v2                  | comments |
|-------------------+------------------------------+---------------------+----------|
| g                 | 0.3                          | {-0.3, 0, 0.1, 0.3} | v2       |
| d                 | {0.01, 0.05, 0.1, 0.15, 0.2} | {0.05, 0.2, 0.35}   |          |
| \pi*                | Small-Big                    | g & d dependent     |          |
| fixation timeout  | skip trial                   | progresses trial    |          |
| stimuli selection | mouse hover                  | mouse click         |          |
** Proposed additions
Uncertainty takes many forms. Visual uncertainty, or the uncertainty that arises when the participants have to approximate the sum of two visually assessed quantities.
* Uncertainty in the Consequential task
Uncertainty
** Visual discrimination/perceptual uncertainty
At least two kinds of uncertainty result from visual perception in the Consequential task.
1. For the smallest d, it can be difficult to determine which stimulus is larger.
2. It can be difficult to visualize and quantify the sum of the two chosen stimuli in an episode.
   - This makes value computation more difficult, which, consequently, makes value comparison between policies more difficult.
** Lack of performance feedback
The lack of performance feedback means participants never know if they are employing the optimal strategy.
** Lack of knowledge regarding which aspects of the stimuli are important
Participants don't know if th



* Test
Some stuff about the plot.
#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt
x = np.random.random((5, 1))
y = np.random.random((5, 1))
plt.plot(x, y)
plt.show()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/1551d142c17728807cb7f7cb3859678e4391e238.png]]
*
