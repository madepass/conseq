@article{fontanesiReinforcementLearningDiffusion2019,
  title = {A Reinforcement Learning Diffusion Decision Model for Value-Based Decisions},
  author = {Fontanesi, Laura and Gluth, Sebastian and Spektor, Mikhail S. and Rieskamp, J{\"o}rg},
  year = {2019},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {4},
  pages = {1099--1121},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-018-1554-2},
  urldate = {2025-02-17},
  abstract = {Psychological models of value-based decision-making describe how subjective values are formed and mapped to single choices. Recently, additional efforts have been made to describe the temporal dynamics of these processes by adopting sequential sampling models from the perceptual decision-making tradition, such as the diffusion decision model (DDM). These models, when applied to value-based decision-making, allow mapping of subjective values not only to choices but also to response times. However, very few attempts have been made to adapt these models to situations in which decisions are followed by rewards, thereby producing learning effects. In this study, we propose a new combined reinforcement learning diffusion decision model (RLDDM) and test it on a learning task in which pairs of options differ with respect to both value difference and overall value. We found that participants became more accurate and faster with learning, responded faster and more accurately when options had more dissimilar values, and decided faster when confronted with more attractive (i.e., overall more valuable) pairs of options. We demonstrate that the suggested RLDDM can accommodate these effects and does so better than previously proposed models. To gain a better understanding of the model dynamics, we also compare it to standard DDMs and reinforcement learning models. Our work is a step forward towards bridging the gap between two traditions of decision-making research.},
  langid = {english},
  file = {/home/maikito/mad/scienctific_articles/Fontanesi et al. - 2019 - A reinforcement learning diffusion decision model for value-based decisions.pdf}
}

@misc{zidHumansForageReward2024,
  title = {Humans Forage for Reward in Reinforcement Learning Tasks},
  author = {Zid, Meriam and Laurie, Veldon-James and {Levine-Champagne}, Alix and Shourkeshti, Akram and Harrell, Dameon and Herman, Alexander B. and Ebitz, R. Becket},
  year = {2024},
  month = jul,
  publisher = {Neuroscience},
  doi = {10.1101/2024.07.08.602539},
  urldate = {2025-01-17},
  abstract = {How do we make good decisions in uncertain environments? In psychology and neuroscience, the classic answer is that we calculate the value of each option and then compare the values to choose the most rewarding, modulo some exploratory noise. An ethologist, conversely, would argue that we commit to one option until its value drops below a threshold, at which point we start exploring other options. In order to determine which view better describes human decision-making, we developed a novel, foraging-inspired sequential decision-making model and used it to ask whether humans compare to threshold (``Forage'') or compare alternatives (``Reinforcement-Learn'' [RL]). We found that the foraging model was a better fit for participant behavior, better predicted the participants' tendency to repeat choices, and predicted the existence of held-out participants with a pattern of choice that was almost impossible under RL. Together, these results suggest that humans use foraging computations, rather than RL, even in classic reinforcement learning tasks.},
  archiveprefix = {Neuroscience},
  langid = {english},
  file = {/home/maikito/mad/scienctific_articles/Zid et al. - 2024 - Humans forage for reward in reinforcement learning tasks.pdf}
}

