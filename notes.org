* currently working on
- [ ] make pavlovia data import function
- [ ] visualize gloria data
- [ ] make function to visualize "correct" answers

- [ ] make functions to determine "learning time"
- [ ] make plot of q-values over time

- [ ] implement empirical data fitting procedure
  - [ ] fit current QLearningAgent to gloria's data

- [ ] make sure environment and real task match as closely as possible
  - [ ] check m

- [ ] ignasi variant - confidence input after each episode

- [ ] statistical power analyses

* todo
** task
- add survey question about input method (eg mouse, trackpad, other)
- double check mouse trajectory polling rate
- make sure online version of task exports full mouse trajectories
- deploy other 3 task versions
** analysis
- environment
  - why only 3 states? need 4
- Q-learning agent
  - optimistic initial values make sense in this case
  - how do q values behave after each g?
  - gamma = 1?
  - softmax choice rule
    - w/ decreasing temperature term over time
      - how should this interact with different G's?
- Q-foraging agent
- C-learning agent
- C-foraging agent
- DDM decision rule
- bayesian parameter optimization
* misc thoughts
- formulate agent for only 1 state?
  - can logically deduce to always choose big in the second trial

* riccardo feedback
- "may have changed" \to "changed"
- maybe make top schematic clearer, indicate repitions more clearly
  - "episode 1", \to "episode 2"
  - think about including in text: "Each episode consists of two trials"
  - "environment 2/4", etc.
- increase episodes from 20 to at least 30
- should mention that only thing that matters is the quantities?
  - riccardo says probably not necessary
** thoughts after riccardo pilot
- risk aversion increases w/ uncertainty
  - make a pure exploration period?
    - period where participants don't lose any â‚¬ for sub-optimal performance
  - should say how many episodes per environment?
    - maybe would influence how explorative they are
